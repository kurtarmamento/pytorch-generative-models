# portable-vae-fashionmnist

A compact, device-agnostic PyTorch implementation of a Convolutional Variational Autoencoder (VAE) trained on Fashion-MNIST (28×28 grayscale).

This project is designed to run with the same training code on:
- CPU (any machine)
- NVIDIA GPU (CUDA build of PyTorch)
- AMD GPU on Windows (DirectML via `torch-directml`, when installed)

## What this project demonstrates
- A complete VAE pipeline: encoder → (mu, logvar) → reparameterization → decoder logits
- Stable training with interpretable metrics: reconstruction loss + KL divergence
- Visualization outputs:
  - reconstructions (input vs reconstructed)
  - unconditional samples (z ~ N(0, I))
  - latent interpolations (mu-space)

## Repository layout
```text
portable-vae-fashionmnist/
  src/
    model.py        # ConvVAE + loss
    train.py        # training loop, device selection, saving, metrics
    viz.py          # image grid + reconstruction visualizations
  data/             # dataset downloaded here (gitignored)
  artifacts/        # images + metrics + best model (gitignored)
  checkpoints/      # optional checkpoints (gitignored)
  README.md
  RESULTS.md
  LICENSE
  .gitignore
```

## Setup (Windows / PyCharm)
Create a virtual environment:
```powershell
py -3.11 -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
```

### Install dependencies (choose ONE backend path)

#### CPU-only (portable baseline)
```powershell
pip install torch torchvision matplotlib tqdm
```

#### NVIDIA (CUDA)
Install PyTorch/torchvision using the official PyTorch selector for your CUDA version, then:
```powershell
pip install matplotlib tqdm
```

#### AMD on Windows (DirectML)
```powershell
pip install torch-directml torchvision matplotlib tqdm
```

Notes:
- If you frequently switch between CPU/CUDA/DirectML, use separate virtual environments to avoid dependency conflicts.
- Fashion-MNIST downloads automatically on first run via torchvision.

## Run

### Smoke test
```powershell
python .\src\train.py --epochs 1 --device auto
```

### Typical training run
```powershell
python .\src\train.py --epochs 20 --batch_size 128 --lr 2e-4 --z_dim 32 --beta 1.0 --beta_warmup_epochs 5 --device auto
```

### Save mode (choose one)

Save only the best epoch (overwrites a single file):
```powershell
python .\src\train.py --epochs 20 --save_best --best_metric total --best_split test
```

Save periodic checkpoints:
```powershell
python .\src\train.py --epochs 20 --save_checkpoints --ckpt_dir checkpoints --save_every 5
```

Resume:
```powershell
python .\src\train.py --resume artifacts\best.pt --epochs 30 --save_best
```

## Outputs
During training, the script writes:

- `artifacts/metrics.csv`
  - Per-epoch averages for: `total`, `recon`, `kl` (and possibly `beta_eff` if you log it)
- `artifacts/recon_epochXXX.png`
  - Input images paired with reconstructions (same image index)
- `artifacts/samples_epochXXX.png`
  - Samples generated by decoding `z ~ N(0, I)`
- `artifacts/interp_epochXXX.png` (if enabled)
  - A row showing latent interpolation between two encoded inputs
- `artifacts/best.pt` (if `--save_best`)
  - Model+optimizer state for the best epoch under your chosen metric

### Metric interpretation (brief)
- `recon`: pixel-summed BCEWithLogits over 28×28 pixels (values in the hundreds are normal)
- `kl`: KL divergence between `q(z|x)` and `N(0, I)` (non-zero indicates latent usage)
- `total = recon + beta * kl`

## This repo includes a GAN implementation
The VAE baseline lives on the `master` branch (tag: `v0.1-vae`).

A GAN implementation is developed on the **`gan` branch**:
- Switch branches on GitHub to `gan` to view the GAN training code and results.
- The `gan` branch adds WGAN training scripts while keeping the VAE baseline intact.
