# PyTorch Generative Models (Fashion-MNIST): VAE + GAN (WIP)

A compact, device-agnostic PyTorch implementation of **generative vision models**, starting with a **Convolutional Variational Autoencoder (VAE)** trained on **Fashion-MNIST (28Ã—28 grayscale)** and expanding to **GAN variants** on a dedicated branch.

This project is designed to run with the same training code on:
- CPU (any machine)
- NVIDIA GPU (CUDA build of PyTorch)
- AMD GPU on Windows (DirectML via `torch-directml`, when installed)

## Quickstart Guide

Simply run train.py from src folder. Results will be saved in the artifacts folder.

Arguments are detailed below for finer adjustment.

## Models and branches

| Model | Status | Where |
|------|--------|-------|
| Convolutional VAE (Fashion-MNIST) | âœ… Implemented | `master` (tag: `v0.1-vae`) |
| GAN / WGAN training | ðŸŸ¡ In progress / expanding | `gan` branch (see branch README + scripts) |

> Rationale: the VAE baseline stays stable on `master`, while GAN work iterates on `gan`.

## What this project demonstrates
- A complete VAE pipeline: encoder â†’ (mu, logvar) â†’ reparameterization â†’ decoder logits
- Stable training with interpretable metrics: reconstruction loss + KL divergence
- Visualization outputs:
  - reconstructions (input vs reconstructed)
  - unconditional samples (z ~ N(0, I))
  - latent interpolations (mu-space)
- Practical reproducibility habits:
  - CLI args for key hyperparameters
  - deterministic seeds (where feasible)
  - artifact logging (metrics + images)

## Repository layout
```text
<repo-root>/
  src/
    model.py        # ConvVAE + loss (VAE baseline)
    train.py        # training loop, device selection, saving, metrics
    viz.py          # image grid + reconstruction visualizations
  data/             # dataset downloaded here (gitignored)
  artifacts/        # images + metrics + best model (gitignored)
  checkpoints/      # optional checkpoints (gitignored)
  README.md
  RESULTS.md
  LICENSE
  .gitignore
```

## Setup (Windows / PyCharm)

Create a virtual environment:
```powershell
py -3.11 -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
```

### Install dependencies (choose ONE backend path)

#### CPU-only (portable baseline)
```powershell
pip install torch torchvision matplotlib tqdm
```

#### NVIDIA (CUDA)
Install PyTorch/torchvision using the official PyTorch selector for your CUDA version, then:
```powershell
pip install torch matplotlib tqdm
```

#### AMD on Windows (DirectML)
```powershell
pip install torch-directml torchvision matplotlib tqdm
```

Notes:
- If you frequently switch between CPU/CUDA/DirectML, use separate virtual environments to avoid dependency conflicts.
- Fashion-MNIST downloads automatically on first run via torchvision.

## Run

### Smoke test
```powershell
python .\src\train.py --epochs 1 --device auto
```

### Typical training run
```powershell
python .\src\train.py --epochs 20 --batch_size 128 --lr 2e-4 --z_dim 32 --beta 1.0 --beta_warmup_epochs 5 --device auto
```

### Save mode (choose one)

Save only the best epoch (overwrites a single file):
```powershell
python .\src\train.py --epochs 20 --save_best --best_metric total --best_split test
```

Save periodic checkpoints:
```powershell
python .\src\train.py --epochs 20 --save_checkpoints --ckpt_dir checkpoints --save_every 5
```

Resume:
```powershell
python .\src\train.py --resume artifacts\best.pt --epochs 30 --save_best
```

## Outputs
During training, the script writes:

- `artifacts/metrics.csv`
  - Per-epoch averages for: `total`, `recon`, `kl` (and possibly `beta_eff` if logged)
- `artifacts/recon_epochXXX.png`
  - Input images paired with reconstructions (same image index)
- `artifacts/samples_epochXXX.png`
  - Samples generated by decoding `z ~ N(0, I)`
- `artifacts/interp_epochXXX.png` (if enabled)
  - A row showing latent interpolation between two encoded inputs
- `artifacts/best.pt` (if `--save_best`)
  - Model+optimizer state for the best epoch under your chosen metric

### Metric interpretation (brief)
- `recon`: pixel-summed BCEWithLogits over 28Ã—28 pixels (values in the hundreds are normal)
- `kl`: KL divergence between `q(z|x)` and `N(0, I)` (non-zero indicates latent usage)
- `total = recon + beta * kl`

## Results
![Sample Epoch](examples/samples_epoch199.png)
![Sample Recon](examples/recon_epoch199.png)
![Sample Interp](examples/interp_epoch199.png)
